{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention\n",
    "\n",
    "Used in Transformer aka [*Attention is all you need*](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).\n",
    "\n",
    "Take the generalised attention format and then instead of comparing src -> tgt, do src -> src, which means that you compare the src to itself. Hence, self attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size for the input\n",
    "seq_len = 50\n",
    "src = torch.arange(seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def self_attention(src: torch.Tensor) -> torch.Tensor:\n",
    "    # modules\n",
    "    src_embed = nn.Embedding(100, 10)\n",
    "    tgt_embed = nn.Embedding(100, 10)\n",
    "\n",
    "    Q = nn.Linear(10, 10)\n",
    "    K = nn.Linear(10, 10)\n",
    "    V = nn.Linear(10, 10)\n",
    "\n",
    "    x = src_embed(src)\n",
    "\n",
    "    q = Q(x)\n",
    "    k = K(x)\n",
    "    v = V(x)\n",
    "\n",
    "\n",
    "    attention_scores = q @ k.T\n",
    "    alpha = attention_scores.softmax(dim=-1)\n",
    "    # attention\n",
    "    a = alpha @ v\n",
    "    return a\n",
    "\n",
    "sa = self_attention(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2446e-02,  2.1148e-02, -4.1858e-01,  7.3752e-02,  1.4769e-01,\n",
       "          3.2192e-02,  3.7492e-02,  1.3095e-01, -1.3525e-01, -1.2915e-01],\n",
       "        [-6.5445e-02,  1.0459e-02, -1.9456e-01,  2.5783e-02,  2.8349e-01,\n",
       "          1.3515e-01,  1.7737e-01,  1.4303e-01, -5.6320e-02, -3.6342e-02],\n",
       "        [-4.5950e-02,  1.4819e-01, -1.5381e-01,  5.2708e-02,  2.4689e-01,\n",
       "          1.0627e-01,  1.2080e-01,  2.4718e-01, -3.6499e-02, -9.3811e-03],\n",
       "        [ 5.1255e-02, -1.9176e-02, -4.8275e-01,  3.5726e-02,  1.5140e-01,\n",
       "          6.3747e-02,  2.5899e-02,  1.0242e-01, -1.2603e-01, -7.2023e-02],\n",
       "        [-1.1805e-01, -6.0078e-02, -1.4068e-01,  1.9195e-02,  1.9841e-01,\n",
       "          1.3663e-01,  1.1511e-01,  2.4002e-01, -3.7578e-05,  7.2950e-02],\n",
       "        [-3.1617e-02,  3.7727e-02, -2.4627e-01,  7.4321e-02,  1.9820e-01,\n",
       "          1.2627e-01,  9.8531e-02,  1.2523e-01, -1.2109e-01, -1.1994e-01],\n",
       "        [-6.4535e-02,  2.8273e-02, -1.2834e-01,  5.8515e-02,  2.3900e-01,\n",
       "          1.8573e-01,  1.7891e-01,  1.3633e-01, -9.1725e-02, -8.0980e-02],\n",
       "        [ 2.6035e-01,  2.1294e-02, -6.8260e-01, -8.8444e-02,  2.4988e-01,\n",
       "         -1.2509e-01, -3.9992e-02,  2.1334e-01,  1.8798e-02,  9.0891e-02],\n",
       "        [-2.6521e-02,  7.5173e-02, -2.5782e-01,  9.2427e-02,  2.1881e-01,\n",
       "          1.5507e-01,  1.3445e-01,  9.9740e-02, -1.3393e-01, -1.3544e-01],\n",
       "        [ 2.2201e-02,  4.2979e-02, -3.0811e-01,  4.2479e-02,  2.1263e-01,\n",
       "          8.8237e-02,  8.4748e-02,  1.2790e-01, -7.3478e-02, -6.7076e-02],\n",
       "        [-1.6154e-03,  4.5410e-02, -2.0118e-01,  5.8158e-02,  1.7634e-01,\n",
       "          8.0614e-02,  5.8055e-02,  1.5807e-01, -1.1512e-01, -1.4612e-01],\n",
       "        [-5.0216e-03, -2.4614e-03, -2.5459e-01,  2.8969e-02,  1.4739e-01,\n",
       "          7.3153e-02,  6.1002e-02,  1.3581e-01, -1.2692e-01, -6.9260e-02],\n",
       "        [-5.9277e-02,  2.7037e-02, -1.9916e-01,  9.0889e-02,  1.8267e-01,\n",
       "          1.3067e-01,  1.0369e-01,  1.3952e-01, -1.2849e-01, -1.1491e-01],\n",
       "        [ 2.3498e-01,  1.0514e-01, -4.9263e-01, -1.3107e-01,  1.7222e-01,\n",
       "          1.6358e-01, -6.1094e-03, -7.5522e-02, -2.8969e-01,  1.8600e-02],\n",
       "        [-9.3682e-02,  8.8993e-02, -2.6157e-01,  1.2104e-01,  2.4471e-01,\n",
       "          1.8241e-01,  1.6135e-01,  5.8008e-02, -1.5751e-01, -1.3200e-01],\n",
       "        [-7.9834e-02,  5.4278e-02, -2.6543e-01,  1.4051e-01,  1.6832e-01,\n",
       "          1.5013e-01,  9.8167e-02,  9.3217e-02, -1.7947e-01, -1.5206e-01],\n",
       "        [-3.7819e-02,  5.7378e-02, -2.2662e-01,  5.1626e-02,  2.7596e-01,\n",
       "          1.6712e-01,  1.3828e-01,  6.7397e-02, -1.3249e-01, -6.9259e-02],\n",
       "        [-4.1060e-02,  9.7902e-02, -2.4546e-01,  3.6590e-02,  2.2463e-01,\n",
       "          9.5057e-02,  4.2132e-02,  1.4174e-01, -1.2252e-01, -8.6706e-02],\n",
       "        [-2.3102e-02, -1.8658e-02, -2.0914e-01,  7.7112e-02,  1.1615e-01,\n",
       "          6.3093e-02,  9.2966e-02,  2.1334e-01, -8.7588e-02, -1.0721e-01],\n",
       "        [-2.0052e-02,  7.0650e-02, -3.1132e-01,  7.0563e-02,  1.9546e-01,\n",
       "          2.2109e-01,  9.8741e-02, -2.6227e-02, -2.3434e-01, -1.5645e-01],\n",
       "        [-2.2094e-03,  1.3937e-01, -3.0194e-01,  9.6939e-02,  1.2796e-01,\n",
       "          2.4925e-01,  1.5854e-01, -7.8361e-02, -3.3176e-01, -2.1679e-01],\n",
       "        [-1.9935e-01, -1.2785e-01, -6.5205e-02,  1.0186e-01,  7.6771e-02,\n",
       "          1.5780e-01,  1.0786e-01,  3.5741e-01,  5.6278e-02,  1.1868e-02],\n",
       "        [-9.3823e-02, -6.3534e-02, -2.1573e-01,  8.5525e-02,  1.4199e-01,\n",
       "          1.0831e-01,  9.7999e-02,  1.8775e-01, -8.6666e-02, -8.4112e-02],\n",
       "        [-8.8831e-03,  2.6440e-02, -3.9573e-01,  1.0134e-01,  1.5428e-01,\n",
       "          8.0197e-02,  6.7028e-02,  1.0854e-01, -1.5344e-01, -1.4866e-01],\n",
       "        [-1.5493e-02, -1.2745e-02, -2.5050e-01,  3.1534e-02,  1.5333e-01,\n",
       "          5.8049e-02,  8.3078e-02,  1.8208e-01, -7.7457e-02, -4.6289e-02],\n",
       "        [-8.5538e-02, -6.6244e-02, -2.3638e-01,  1.0682e-01,  7.8062e-02,\n",
       "          9.5136e-02,  6.6960e-02,  2.2969e-01, -8.0750e-02, -8.3694e-02],\n",
       "        [-1.2183e-01,  4.1726e-02, -1.5910e-01,  2.8157e-02,  3.2048e-01,\n",
       "          1.8441e-01,  1.4186e-01, -1.6902e-02, -2.2112e-01, -7.8980e-02],\n",
       "        [ 5.7586e-02,  2.0440e-02, -4.4476e-01,  2.8175e-02,  1.8792e-01,\n",
       "          6.0419e-02,  3.3557e-02,  6.3664e-02, -1.6613e-01, -1.2416e-01],\n",
       "        [ 8.8090e-02,  2.0449e-02, -4.0251e-01, -6.5779e-03,  1.5358e-01,\n",
       "          1.1749e-01,  5.8545e-02,  5.7868e-02, -1.7715e-01, -4.3876e-02],\n",
       "        [-4.6984e-02,  1.2232e-02, -2.5464e-01,  8.5804e-02,  1.7820e-01,\n",
       "          1.3309e-01,  1.0361e-01,  1.1366e-01, -1.3546e-01, -1.2127e-01],\n",
       "        [-9.1430e-03,  2.3417e-02, -2.3631e-01,  1.9484e-02,  1.6733e-01,\n",
       "          1.0676e-01,  5.4460e-02,  9.3878e-02, -1.8600e-01, -9.8796e-02],\n",
       "        [-9.2175e-02,  8.8079e-02, -2.9113e-01,  1.2911e-01,  2.2305e-01,\n",
       "          2.4462e-01,  8.3082e-02, -6.7406e-02, -2.4410e-01, -2.0198e-01],\n",
       "        [ 3.5550e-01,  1.0278e-01, -5.6297e-01, -2.0469e-01,  1.6561e-01,\n",
       "          7.0572e-02, -4.7528e-03,  2.8106e-02, -2.0666e-01,  1.6032e-01],\n",
       "        [ 7.3919e-02,  1.2262e-01, -2.5870e-01, -4.7038e-02,  1.4373e-01,\n",
       "          3.2885e-01,  1.5275e-01, -1.7849e-01, -4.0172e-01, -1.2488e-01],\n",
       "        [-6.9943e-02, -2.6182e-04, -2.2455e-01,  7.8434e-02,  1.8864e-01,\n",
       "          1.2937e-01,  1.0899e-01,  1.3707e-01, -1.1245e-01, -8.3180e-02],\n",
       "        [-5.3886e-02,  9.2958e-02, -2.9143e-01,  1.0833e-01,  2.4991e-01,\n",
       "          1.2557e-01,  1.6611e-01,  6.9579e-02, -1.6360e-01, -1.1528e-01],\n",
       "        [-3.3061e-02,  3.1270e-03, -3.2128e-01,  6.4157e-02,  1.9554e-01,\n",
       "          1.5779e-01,  7.5229e-02,  8.1341e-02, -1.4059e-01, -6.5174e-02],\n",
       "        [-7.6349e-02,  9.3987e-02, -2.3340e-01,  1.0685e-01,  2.4794e-01,\n",
       "          1.7290e-01,  1.3099e-01,  8.5748e-02, -1.5964e-01, -9.8973e-02],\n",
       "        [-5.9118e-02,  1.2963e-01, -2.7634e-01,  1.1094e-01,  2.4732e-01,\n",
       "          1.8472e-01,  1.2039e-01,  5.6488e-02, -1.4962e-01, -1.4474e-01],\n",
       "        [-7.0217e-02,  1.4798e-01, -2.3548e-01,  1.0700e-01,  2.4954e-01,\n",
       "          2.4945e-01,  1.5547e-01, -4.3374e-03, -2.0210e-01, -1.5822e-01],\n",
       "        [ 8.8038e-02,  2.4593e-02, -4.7891e-01,  1.2108e-03,  2.3411e-01,\n",
       "          3.1987e-02,  4.2239e-02,  1.5826e-01, -2.2266e-02,  2.6480e-02],\n",
       "        [-2.7251e-02,  9.9364e-02, -3.0950e-01,  7.8951e-02,  2.3132e-01,\n",
       "          1.2869e-01,  4.0980e-02,  6.9607e-02, -1.5109e-01, -1.7208e-01],\n",
       "        [ 3.4218e-02,  1.2970e-02, -4.0351e-01,  6.3517e-02,  1.4823e-01,\n",
       "          5.3762e-02,  5.0350e-02,  1.2799e-01, -1.2974e-01, -1.1390e-01],\n",
       "        [-1.5098e-02,  1.0543e-01, -3.6737e-01,  9.3380e-02,  1.9325e-01,\n",
       "          2.3304e-01,  5.0092e-02, -1.0726e-01, -2.9350e-01, -2.3285e-01],\n",
       "        [ 2.4394e-01,  1.6767e-01, -2.5861e-01, -1.8059e-01,  6.6550e-02,\n",
       "          3.7174e-01,  1.2018e-01, -2.3705e-01, -5.0445e-01,  1.9006e-03],\n",
       "        [ 5.7183e-02, -4.9851e-02, -2.1392e-01, -4.2653e-02,  1.0193e-01,\n",
       "          5.8741e-02,  5.4787e-02,  1.1875e-01, -1.7551e-01, -5.8762e-02],\n",
       "        [-1.1597e-01,  1.3641e-02, -1.5470e-01,  6.0163e-02,  2.6812e-01,\n",
       "          1.2975e-01,  1.5536e-01,  1.5267e-01, -1.1941e-01, -2.6357e-02],\n",
       "        [-3.3702e-02, -7.8745e-03, -3.0971e-01,  6.0377e-02,  1.9059e-01,\n",
       "          1.0699e-01,  9.6673e-02,  1.6136e-01, -6.6790e-02, -4.7185e-02],\n",
       "        [ 5.0859e-01,  7.7113e-03, -9.6307e-01, -2.6982e-01,  3.3149e-01,\n",
       "         -3.3951e-01, -1.5673e-01,  3.7666e-01,  1.9562e-01,  4.0318e-01],\n",
       "        [ 2.5398e-01, -1.4957e-02, -6.6715e-01, -1.0417e-01,  2.0564e-01,\n",
       "         -1.2859e-01, -6.1855e-02,  3.3352e-01,  1.2035e-01,  2.1358e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled self attention\n",
    "\n",
    "Self attention is based on dot product between the key and the query. Problem is, dot products can get very large at times. Suppose the embedding dimension $d_k$ was about 512, then, the dot product can be as large as 512. Increase the size and we may have a computation problem. Furthermore, larger values don't play well with softmax, especially if not scaled. (Softmax provides a probability dist. after all and that can get skewed.)\n",
    "\n",
    "So in the Transformer paper, they just scale the attention scores before applying softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def scaled_self_attention(src: torch.Tensor) -> torch.Tensor:\n",
    "    src_embed = nn.Embedding(100, 512)\n",
    "    x = src_embed(src)\n",
    "    # scaling factor\n",
    "    dk = x.size(-1)\n",
    "\n",
    "    # modules\n",
    "    Q = nn.Linear(dk, 100)\n",
    "    K = nn.Linear(dk, 100)\n",
    "    V = nn.Linear(dk, 100)\n",
    "\n",
    "    \n",
    "\n",
    "    q = Q(x)\n",
    "    k = K(x)\n",
    "    v = V(x)\n",
    "\n",
    "\n",
    "    # scale\n",
    "    attention_scores = (q @ k.T) / torch.sqrt(torch.tensor(dk))\n",
    "    alpha = attention_scores.softmax(dim=-1)\n",
    "    # attention\n",
    "    a = alpha @ v\n",
    "    return a\n",
    "\n",
    "ssa = scaled_self_attention(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0383,  0.0145,  0.0007,  ..., -0.0889,  0.0965, -0.0302],\n",
       "        [-0.0370,  0.0450, -0.0110,  ..., -0.0941,  0.1030, -0.0178],\n",
       "        [-0.0371,  0.0136,  0.0092,  ..., -0.1074,  0.0901, -0.0361],\n",
       "        ...,\n",
       "        [-0.0302,  0.0214,  0.0121,  ..., -0.0812,  0.0733, -0.0547],\n",
       "        [-0.0642,  0.0256,  0.0020,  ..., -0.0957,  0.0789, -0.0306],\n",
       "        [-0.0639,  0.0295, -0.0151,  ..., -0.1058,  0.0872, -0.0257]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssa.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
