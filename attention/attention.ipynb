{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau et al. (2016)\n",
    "\n",
    "https://arxiv.org/abs/1409.0473\n",
    "\n",
    "https://machinelearningmastery.com/the-bahdanau-attention-mechanism/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a sequence (usually text) of n elements. So it looks like this -\n",
    "\n",
    "$$ X = x_1 , x_2 , ...... x_n $$\n",
    "\n",
    "Which needs to be aligned with another sequence -\n",
    "\n",
    "$$ Y = y_1 , y_2 , ...... y_n $$\n",
    "\n",
    "And an encoder decoder network which in the paper is based on a bi-directional RNN. Traditional encoders create a fixed size vector from an input sequence, which limits the amount of information the decoder can use. Furthermore, the encoder has to create this vector from multi-dim hidden states, which may result in loss of information. \n",
    "\n",
    "What attention does is - \n",
    "- Finds the probability of an element in $X$, $x_j$ to be aligned with an element in the output sequence, $Y$, $y_i$\n",
    "- The original work was for Machine Translation, hence the alignment modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So say the hidden state from the encoder is $h_j$ and from the decoder, $s_{i - 1}$\n",
    "\n",
    "Then, \n",
    "\n",
    "attention probability for $y_i$ to $x_j$ is $\\alpha_{ij} = softmax(e_{ij})$ (softmax since probability!)\n",
    "\n",
    "now what is $e$ here? Well actually, the hidden state of the encoder contains a representations for all elements in $X$, and attention just doesn't look at one element on both sequences and finds a probability. It compares one element in the output against all the others in the input and says, how related is this element at $i$ in the output to $1....j$ elements in the input?\n",
    "\n",
    "Why from output to input? Machine Translation, you're finding a proper translation for a source sentence. Or in other words, you want your decoder to search for the proper translation. Attention provides the search information for the decoder. So moving on, $e$, or the attention score comes from the attention module, which is defined and trained as a feed forward network.\n",
    "\n",
    "$e_{ij} = a(s_{i -1}, h_j)$\n",
    "\n",
    "And then, a context vector $c_i$ is created from $\\alpha_{ij}$\n",
    "\n",
    "$$\n",
    " c_i = \\sum_{j = 1}^{n} \\alpha_{ij}h_j\n",
    "$$\n",
    "\n",
    "In layman terms, find how probable $y_i$ is to be an output candidate based on the entire input $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to be more specific, \n",
    "\n",
    "input goes to encoder -> gives hidden state $h_j$\n",
    "\n",
    "$h_j$ goes to alignment module with the previous decoder state $s_{i - 1}$ -> gives attention score $e_{ij}$\n",
    "\n",
    "take softmax of the attention score\n",
    "\n",
    "get the context vector by weighted sum of annotation score and encoder hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random tensors for the hidden states of encoder and the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy input of size 50\n",
    "x = torch.arange(50)\n",
    "\n",
    "embedding = nn.Embedding(100, 20)\n",
    "encoder = nn.RNN(20, 30, bidirectional=True)\n",
    "decoder = nn.RNN(30, 30)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = embedding(x)\n",
    "    _, h_j = encoder(x)\n",
    "\n",
    "\n",
    "s_previous = h_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to get the attention score using the hidden states. \n",
    "\n",
    "1. Concatenate both and pass to a FFN with a single weight, $W$ (dot product)\n",
    "2. Or, have two weights for each hidden state, apply the weights individually in a FFN and then add them (additive). \n",
    "\n",
    "in both cases the output of the FFN is passed to a tanh activation layer and then, a weight vector is applied for scaling.\n",
    "\n",
    "The weight vectors are implemented as linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def attention(s_previous: torch.Tensor, h_j: torch.Tensor,  mode=\"dot\") -> torch.Tensor:    \n",
    "    if mode == \"dot\":\n",
    "        # concat\n",
    "        hidden_states = torch.cat([h_j, s_previous], dim=-1)\n",
    "        \n",
    "        # init weight\n",
    "        # actually use a linear layer with no bias\n",
    "        # output should be the size of h_j\n",
    "        # well you have to multiply the attention score softmax with h_j!\n",
    "        W = nn.Linear(hidden_states.size(-1), h_j.size(-1))\n",
    "        \n",
    "        # e_ij\n",
    "        attention_scores = F.tanh(W(hidden_states))\n",
    "        \n",
    "        # scale by  weight vector\n",
    "        V = nn.Linear(attention_scores.size(-1), 1)\n",
    "        attention_scores = V(attention_scores)\n",
    "  \n",
    "    elif mode == \"add\":\n",
    "        W1 = nn.Linear(h_j.size(-1), h_j.size(-1))\n",
    "        W2 = nn.Linear(s_previous.size(-1), s_previous.size(-1))\n",
    "        \n",
    "        attention_scores = F.tanh(W1(h_j) + W2(s_previous))\n",
    "        \n",
    "        V = nn.Linear(attention_scores.size(-1), 1)\n",
    "        attention_scores = V(attention_scores)\n",
    "        \n",
    "    \n",
    "    # transpose, since matmul for context vector\n",
    "    attention_scores = attention_scores.T\n",
    "    alpha = F.softmax(attention_scores, dim=-1)\n",
    "    c = alpha @ h_j\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1915, -0.7699,  0.0801,  0.0558,  0.6161,  0.0018,  0.2433,  0.1940,\n",
      "         -0.2244,  0.4273,  0.1298, -0.3667, -0.0507, -0.4133,  0.0799,  0.5204,\n",
      "          0.2218,  0.0478, -0.0245,  0.6666,  0.2871,  0.4599,  0.4257,  0.6712,\n",
      "          0.0661, -0.1901, -0.1643, -0.4719,  0.1610, -0.5516]])\n",
      "torch.Size([1, 30])\n"
     ]
    }
   ],
   "source": [
    "# concat\n",
    "ctx = attention(s_previous, h_j)\n",
    "print(ctx)\n",
    "print(ctx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2746, -0.7863,  0.0674,  0.0451,  0.6449,  0.0602,  0.3130,  0.1456,\n",
      "         -0.2343,  0.4156,  0.0650, -0.3128, -0.0150, -0.3874,  0.1069,  0.4908,\n",
      "          0.2967,  0.0543, -0.0060,  0.6706,  0.3313,  0.4690,  0.3695,  0.6550,\n",
      "          0.0606, -0.1467, -0.1490, -0.5003,  0.1544, -0.5247]])\n",
      "torch.Size([1, 30])\n"
     ]
    }
   ],
   "source": [
    "# additive\n",
    "ctx = attention(s_previous, h_j, \"add\")\n",
    "print(ctx)\n",
    "print(ctx.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
