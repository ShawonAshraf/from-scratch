{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT, OptimizerLRScheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import lightning.pytorch as L\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = FashionMNIST(\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are all integers and their string descriptors are detailed here: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
    "\n",
    "(Why not just add them as metadata to the torchvision dataset?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(trainset[0][0].squeeze(), cmap=\"gray\")\n",
    "print(trainset[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=128, kernel_size=3, padding=1, stride=2\n",
    "        )\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.norm1 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=64, kernel_size=3, padding=1, stride=2\n",
    "        )\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=32, kernel_size=3, padding=1, stride=2\n",
    "        )\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.norm3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.activation1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.activation3(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_inputs = torch.randn(1, 1, 28, 28)\n",
    "\n",
    "enc = Encoder()\n",
    "with torch.no_grad():\n",
    "    enc_out = enc(dummy_inputs)\n",
    "    print(enc_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_out.flatten().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(\n",
    "            32, 64, 3, padding=1, stride=2, output_padding=0\n",
    "        )\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.trans_conv2 = nn.ConvTranspose2d(\n",
    "            64, 128, 3, padding=1, stride=2, output_padding=1\n",
    "        )\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.trans_conv3 = nn.ConvTranspose2d(\n",
    "            128, 1, 3, padding=1, stride=2, output_padding=1\n",
    "        )\n",
    "        self.norm3 = nn.BatchNorm2d(1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        out = self.norm1(F.leaky_relu(self.trans_conv1(x)))\n",
    "        out = self.norm2(F.leaky_relu(self.trans_conv2(out)))\n",
    "        out = self.norm3(F.leaky_relu(self.trans_conv3(out)))\n",
    "\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "dec = Decoder()\n",
    "with torch.no_grad():\n",
    "    dec_out = dec(enc_out)\n",
    "    print(dec_out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dummy_inputs.size() == dec_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian factorised VAE\n",
    "class VAE(L.LightningModule):\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        # latent space\n",
    "        self.mean = nn.Linear(512, 512)\n",
    "        self.log_variance = nn.Linear(512, 512)\n",
    "\n",
    "        # project to previous size\n",
    "        # self.projection = nn.Linear(384, 512)\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 4, 4))\n",
    "\n",
    "    def reparameterisation(self, mean, log_variance):\n",
    "        std_dev = torch.exp(0.5 * log_variance)\n",
    "\n",
    "        epsilon = torch.randn_like(std_dev).to(self.device)\n",
    "        epsilon.requires_grad = True\n",
    "\n",
    "        return mean + epsilon * std_dev\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Any:\n",
    "        encoder_out = self.encoder(x)\n",
    "        encoder_out = self.flatten(encoder_out)\n",
    "\n",
    "        mean = F.leaky_relu(self.mean(encoder_out))\n",
    "        log_variance = F.leaky_relu(self.log_variance(encoder_out))\n",
    "\n",
    "        z = self.reparameterisation(mean, log_variance)\n",
    "        # z = self.projection(z)\n",
    "        # z = F.leaky_relu(z)\n",
    "        z = self.unflatten(z)\n",
    "\n",
    "        decoder_out = self.decoder(z)\n",
    "\n",
    "        # x_hat, mean, log_sigma_2\n",
    "        return decoder_out, mean, log_variance\n",
    "\n",
    "    def configure_optimizers(self) -> OptimizerLRScheduler:\n",
    "        return optim.AdamW(self.parameters(), lr=0.0005)\n",
    "\n",
    "    def training_step(self, batch, batch_idx) -> STEP_OUTPUT:\n",
    "        x, _ = batch\n",
    "        x_hat, mean, log_variance = self(x)\n",
    "\n",
    "        loss = F.binary_cross_entropy(x_hat, x, reduction=\"sum\")\n",
    "        kl = -0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp())\n",
    "        loss += kl\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return {\"loss\": loss, \"log\": {\"Loss/Training\": loss}}\n",
    "\n",
    "\n",
    "# vae = VAE()\n",
    "# with torch.no_grad():\n",
    "#     out, mean, log_variance = vae(dummy_inputs)\n",
    "#     print(out.size())\n",
    "\n",
    "\n",
    "def train() -> Any:\n",
    "    logger = L.loggers.TensorBoardLogger(\n",
    "        \"tb_logs\", name=\"vae_cnn_fashion_mnist\", log_graph=True\n",
    "    )\n",
    "\n",
    "    model = VAE()\n",
    "    trainer = L.Trainer(max_epochs=5, devices=1, accelerator=\"gpu\", logger=logger)\n",
    "    trainer.fit(model, trainloader)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.randn(1, 1, 28, 28).to(model.device) / 255.0\n",
    "    out, _, _ = model(x)\n",
    "\n",
    "    print(out.size())\n",
    "\n",
    "    plt.imshow(out.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = trainset[0][0].unsqueeze(0).to(model.device)\n",
    "    out, _, _ = model(x)\n",
    "\n",
    "    print(out.size())\n",
    "\n",
    "    plt.imshow(out.squeeze(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer(x, model=model):\n",
    "    x = x.to(model.device)\n",
    "    out, _, _ = model(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(idx):\n",
    "    fig = plt.figure(1, figsize=[9, 5])\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # get 3 generated images\n",
    "    vae_images = [infer(trainset[idx][0].unsqueeze(0)) for _ in range(3)]\n",
    "\n",
    "    ax = fig.add_subplot(1, 4, 1)\n",
    "    ax.set_title(\"Actual\")\n",
    "    plt.imshow(trainset[idx][0].squeeze(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    for im_idx, vae_im in enumerate(vae_images):\n",
    "        ax = fig.add_subplot(1, 4, im_idx + 1 + 1)\n",
    "        ax.set_title(f\"VAE_{im_idx}\")\n",
    "        plt.imshow(vae_im.squeeze(), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Actual vs VAE\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    comparison(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
