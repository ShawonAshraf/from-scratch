{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "A far easier way to understand the changes in sLSTM and mLSTM in xLSTM would be to first look at an LSTM cell and then adding the modifications over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ecd8f10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from loguru import logger\n",
    "\n",
    "torch.manual_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3048e-01, -3.4990e-01,  4.7494e-01,  9.0407e-01, -7.0212e-01],\n",
       "         [ 1.5963e+00,  4.2280e-01, -6.9397e-01,  9.6718e-01,  1.5569e+00],\n",
       "         [-2.3860e+00,  6.9941e-01, -1.0325e+00, -2.6043e+00,  9.3368e-01],\n",
       "         [-1.0496e-01,  7.4267e-01, -1.3397e+00, -3.6486e-01,  2.5399e-01],\n",
       "         [-1.4082e+00,  2.8347e-01, -9.3333e-01, -6.2785e-01, -7.5152e-02],\n",
       "         [-2.2086e+00, -1.1256e+00,  2.4818e-02,  1.2566e+00, -9.3699e-01],\n",
       "         [ 4.8638e-02,  2.8411e-01, -9.5578e-01,  1.4745e+00,  5.1086e-01],\n",
       "         [-2.3249e-01,  3.9579e-01,  8.5357e-01, -4.2040e-01, -1.4516e+00],\n",
       "         [-7.3737e-01, -4.2015e-01,  3.0709e-01, -1.2767e+00,  2.0085e-01],\n",
       "         [ 1.8960e-02,  3.0411e-01, -9.2130e-01,  4.0975e-01, -1.5108e+00]],\n",
       "\n",
       "        [[ 2.9006e-01,  2.5075e+00, -8.9630e-01, -2.2588e+00, -2.2113e-01],\n",
       "         [-1.6946e+00, -2.8795e-01, -6.5329e-01,  1.3445e+00, -3.7231e-01],\n",
       "         [-6.5886e-01, -2.3493e-01,  5.0538e-01,  1.8711e+00, -1.6772e+00],\n",
       "         [ 1.3262e+00,  1.1294e+00,  1.2962e+00,  6.1683e-01,  2.4782e-01],\n",
       "         [-1.4798e-03,  1.2057e+00, -5.2975e-02,  1.3748e+00,  9.2350e-01],\n",
       "         [-2.2113e+00, -8.9604e-01,  5.9713e-01, -2.1406e-01,  2.1855e+00],\n",
       "         [ 1.2477e+00, -2.6657e-01,  1.6528e+00, -6.8877e-02,  1.0442e+00],\n",
       "         [-4.4236e-01, -8.4945e-01, -5.7722e-01, -1.0064e-01,  6.6707e-01],\n",
       "         [-1.6043e-01, -2.1008e+00,  1.9666e+00, -6.3452e-01, -1.6574e+00],\n",
       "         [-8.4563e-02,  7.9562e-02, -9.2849e-01, -7.9525e-01,  9.9244e-01]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy input sequence of integers. 2 sequences actually.\n",
    "\n",
    "x = torch.randn(2, 10, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the base implementation of lstm in pytorch\n",
    "\n",
    "hidden_size = 5\n",
    "input_size = x.size(-1)\n",
    "\n",
    "# input size, hidden size, num layers\n",
    "rnn = nn.LSTM(input_size, hidden_size, 1)\n",
    "\n",
    "# h_0 size should be n, input_size, hidden size\n",
    "h_0 = torch.randn(1, 10, hidden_size)\n",
    "c_0 = torch.randn(h_0.size())\n",
    "\n",
    "out, (hn, cn) = rnn(x.float(), (h_0, c_0))\n",
    "out.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the weight and bias kernels for the LSTM cell\n",
    "\n",
    "The LSTM cell has 4 sets of weights and biases. The weights are for the input, forget, output and cell state respectively. The biases are for the input, forget, output and cell state respectively.\n",
    "\n",
    "The weights are defined as follows:\n",
    "- $W_{xi}$: Weights for the input gate\n",
    "- $W_{xf}$: Weights for the forget gate\n",
    "- $W_{xo}$: Weights for the output gate\n",
    "- $W_{xc}$: Weights for the cell state\n",
    "\n",
    "The biases are defined as follows:\n",
    "- $b_{i}$: Bias for the input gate\n",
    "- $b_{f}$: Bias for the forget gate\n",
    "- $b_{o}$: Bias for the output gate\n",
    "- $b_{c}$: Bias for the cell state\n",
    "\n",
    "### Defining the LSTM cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-09-12 02:05:07.945\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mx :: torch.Size([2, 10, 5])\u001b[0m\n",
      "\u001b[32m2024-09-12 02:05:07.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mh_0 :: torch.Size([1, 10, 5])\u001b[0m\n",
      "\u001b[32m2024-09-12 02:05:07.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mforward\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mc_0 :: torch.Size([1, 10, 5])\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x10 and 5x5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     91\u001b[0m     lstm \u001b[38;5;241m=\u001b[39m LSTMCell(input_size, hidden_size)\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/from-scratch-iDq3ZbAM-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/from-scratch-iDq3ZbAM-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 70\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, x, h_0, c_0)\u001b[0m\n\u001b[1;32m     66\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh_0 :: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_0\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_0 :: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc_0\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m z_t \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_z\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_z \u001b[38;5;241m@\u001b[39m h_0) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_z\n\u001b[1;32m     71\u001b[0m z_t \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mtanh(z_t)\n\u001b[1;32m     73\u001b[0m i_t \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_i\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m x) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_i \u001b[38;5;241m@\u001b[39m h_0) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_i\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x10 and 5x5)"
     ]
    }
   ],
   "source": [
    "# an unidirectional lstm cell (single layer)\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # cell output gate\n",
    "        self.W_z = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        \n",
    "        # input gate\n",
    "        self.W_i = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        \n",
    "        # forget gate\n",
    "        self.W_f = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        \n",
    "        # output gate\n",
    "        self.W_o = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        \n",
    "        # biases\n",
    "        self.b_z = nn.Parameter(\n",
    "            torch.empty(self.hidden_size)\n",
    "        )\n",
    "        self.b_i = nn.Parameter(\n",
    "            torch.empty(self.hidden_size)\n",
    "        )\n",
    "        self.b_f = nn.Parameter(\n",
    "            torch.empty(self.hidden_size)\n",
    "        )\n",
    "        self.b_o = nn.Parameter(\n",
    "            torch.empty(self.hidden_size)\n",
    "        )\n",
    "        \n",
    "        # hidden / recurrent weights\n",
    "        # doesn't have bias\n",
    "        self.r_z = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        self.r_i = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        self.r_f = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "        self.r_o = nn.Parameter(\n",
    "            torch.empty(self.hidden_size, self.input_size)\n",
    "        )\n",
    "    \n",
    "    # TODO: implement later\n",
    "    def init_params(self):\n",
    "        pass\n",
    "    \n",
    "    # TODO: fix dimension mismatch error\n",
    "    def forward(self, x, h_0, c_0):\n",
    "        logger.info(f\"x :: {x.size()}\")\n",
    "        logger.info(f\"h_0 :: {h_0.size()}\")\n",
    "        logger.info(f\"c_0 :: {c_0.size()}\")\n",
    "        \n",
    "        \n",
    "        z_t = (self.W_z.T @ x) + (self.r_z @ h_0) + self.b_z\n",
    "        z_t = F.tanh(z_t)\n",
    "        \n",
    "        i_t = (self.W_i.T @ x) + (self.r_i @ h_0) + self.b_i\n",
    "        i_t = F.sigmoid(i_t)\n",
    "        \n",
    "        f_t = (self.W_f.T @ x) + (self.r_f @ h_0) + self.b_f\n",
    "        f_t = F.sigmoid(f_t)\n",
    "        \n",
    "        o_t = (self.W_o.T @ x) + (self.r_o @ h_0) + self.b_o\n",
    "        o_t = F.sigmoid(o_t)\n",
    "        \n",
    "        # new cell state\n",
    "        c_t = f_t @ c_0 + i_t @ z_t\n",
    "        # new hidden state\n",
    "        h_t = o_t @ F.tanh(c_t)\n",
    "        \n",
    "        return z_t, c_t, h_t\n",
    "    \n",
    "    \n",
    "with torch.no_grad():\n",
    "    lstm = LSTMCell(input_size, hidden_size)\n",
    "    out = lstm(x, h_0, c_0)\n",
    "    print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
