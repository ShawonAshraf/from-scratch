{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sLSTM is one of the components of an xLSTM stack. There are two components actually, sLSTM, and mLSTM. Both solve problems with vanilla LSTM regarding their sequential nature, complete lack of parallelisation, memory mixing (or parameter sharing between cells), local updates (instead of going through all the elements in a sequence) and compressed scalar carousel (or carry or cell state). Let's look at sLSTM first which tackles the following issues in a vanilla LSTM cell: \n",
    "\n",
    "1. Memory mixing between cells - a cell can have multiple parameters but they won't be shared with other cells. So the memory mixing happens within the cell. \n",
    "2. Non scalar cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "A far easier way to understand the changes in sLSTM would be to first look at an LSTM cell and the adding the sLSTM modifications over it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 4, 8, 8, 7, 2, 8, 0, 9],\n",
       "        [9, 4, 4, 7, 3, 2, 4, 7, 7, 6]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy input sequence of integers. 2 sequences actually.\n",
    "\n",
    "x = torch.randint(0, 10, (2, 10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial hidden and cell state\n",
    "# say that the hidden size is 5\n",
    "# can also be 0s as set in the pytorch LSTM module\n",
    "h_0 = torch.randn(5)\n",
    "c_0 = torch.randn(h_0.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def lstm(x, h_0, c_0):\n",
    "    # embedding vectors\n",
    "    x_embed = nn.Embedding(embedding_dim=10, num_embeddings=10)(x)\n",
    "\n",
    "    \n",
    "    for t in range(10):\n",
    "        \n",
    "        # input layer\n",
    "        z_t = F.tanh(x_embed)\n",
    "    \n",
    "        # input gate\n",
    "    \n",
    "        # forget gate\n",
    "    \n",
    "        # output gate\n",
    "    \n",
    "        # cell state\n",
    "    \n",
    "        # hidden state\n",
    "\n",
    "    \n",
    "    return z_t\n",
    "\n",
    "\n",
    "lstm(x).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
